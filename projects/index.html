<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>RAY'S SPACE</title><meta name="author" content="Rui Cao (Ray)"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css"><meta name="generator" content="Hexo 4.2.1"></head><body><header id="page_header"><div class="header_wrap"><div id="blog_name"><a class="blog_title" id="site-name" href="/">RAY'S SPACE</a></div><button class="menus_icon"><div class="navicon"></div></button><ul class="menus_items"><li class="menus_item"><a class="site-page" href="/"> HOME</a></li><li class="menus_item"><a class="site-page" href="/publications"> PUBLICATIONS</a></li><li class="menus_item"><a class="site-page" href="/blog"> BLOG</a></li><li class="menus_item"><a class="site-page" href="/about"> ABOUT</a></li></ul></div></header><main id="page_main"><div class="side-card sticky"><div class="card-wrap" itemscope itemtype="http://schema.org/Person"><div class="author-avatar"><img class="avatar-img" src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/profile.png'" alt="avatar"></div><div class="author-discrip"><h3>Rui Cao (Ray)</h3><p class="author-bio">PhD candidate @UoN &amp; Research assistant @SZU</p></div><div class="author-links"><button class="btn m-social-links">Links</button><ul class="social-icons"><li><a class="social-icon" href="mailto:rcao@outlook.com" target="_blank"><i class="fas fa-envelope" aria-hidden="true"></i></a></li></ul><ul class="social-links"><li><a class="e-social-link" href="https://scholar.google.com/citations?user=Goj1ZjsAAAAJ" target="_blank"><i class="ai ai-google-scholar-square" aria-hidden="true"></i><span> Google Scholar</span></a></li><li><a class="e-social-link" href="https://www.researchgate.net/profile/Rui_Cao54" target="_blank"><i class="ai ai-researchgate-square" aria-hidden="true"></i><span> ResearchGate</span></a></li><li><a class="e-social-link" href="https://orcid.org/0000-0002-1440-4175" target="_blank"><i class="ai ai-orcid-square" aria-hidden="true"></i><span> ORCID</span></a></li></ul></div></div></div><div class="page" itemscope itemtype="http://schema.org/CreativeWork"><h2 class="page-title">Projects</h2><article><!--
## Urban land use classification using multi-source geospatial data
This project integrates remote and social sensing data for urban land use classification.
-->


<p>09/2017 - present, Guangdong Key Laboratory of Urban Informatics &amp; Guangdong Key Laboratory of Intelligent Information Processing, Shenzhen University, China. Research Assistant (Supervised by Prof. Guoping Qiu and Prof. Qingquan Li)</p>
<ul>
<li><p><strong>Integrating high-resolution remote sensing images with multi-source geospatial big data for complex urban land use classification using deep learning (leading project)</strong><br>Develop deep learning-based methods to fuse high resolution remote sensing images with multi-source and multi-modal geospatial big data (such as street view images, social media data, points of interest (POI), and mobile phone positioning data) for urban land use classification and functional zone recognition. Part of the research outputs have already been published on ISPRS Journal of Photogrammetry and Remote Sensing and Remote Sensing; and part of the outputs have been presented in the International Conference on Content-Based Multimedia Indexing (CBMI).</p>
</li>
<li><p><strong>Cross-view image matching and retrieval using deep metric learning (leading project)</strong><br>Develop deep learning-based methods to effectively match remote sensing/cross-view images for image matching and retrieval purposes, as well as their applications in geolocalization. Part of the research outputs have been published on International Journal of Remote Sensing; and part of the outputs are presented in the International Conference on Image and Graphics (ICIG).</p>
</li>
<li><p><strong>Image-based indoor localization using deep learning (participated project)</strong><br>Develop deep learning-based methods for image-based indoor localization, including both topological and metric localization. Part of the research outputs have been published on ISPRS Journal of Photogrammetry and Remote Sensing and Remote Sensing; and part of the products are under review on Neurocomputing.</p>
</li>
<li><p><strong>Monocular depth estimation using deep learning (participated project)</strong><br>Develop deep learning-based methods for image-based monocular depth estimation, using both supervised and unsupervised learning techniques. Part of the research outputs have been submitted to ISPRS Journal of Photogrammetry and Remote Sensing and Image and Vision Computing under review.</p>
</li>
</ul>
<p>12/2016 - 06/2017, School of Computer Science &amp; Horizon Digital Economy Research, University of Nottingham, UK. PhD Student (Supervised by Prof. Guoping Qiu, Dr. Gavin Smith, and Dr. James Goulding)</p>
<ul>
<li><strong>Consumer spending behaviour analysis &amp; retailers localization based on mobile money data (participated project)</strong><br>Analyse economic activities and consumer spending behaviour based on mobile money transaction records. Based on those patterns, distinguish retailers from non-sellers, and locate the retailers. The research output has been submitted as module report.</li>
</ul>
<p>07/2014 - 06/2016, Shenzhen Key Laboratory of Spatial Smart Sensing and Services, Shenzhen University, China. Research Assistant (Supervised by Prof. Qingquan Li and Dr. Wei Tu)</p>
<ul>
<li><p><strong>Research on the identification of urban function zones and analysis of human mobility patterns based on big transportation data (leading &amp; participated project)</strong><br>Process multi-source big transportation data, including transit smart card data, vehicle GPS trajectory data, etc. Extract trips from those data, analyse human mobility patterns of different modes of transportation, and identify urban function zones based on them. Part of the output of the project was presented on International Conference on Computers in Urban Planning and Urban Management (CUPUM), International Geographical Congress, and ISPRS Congress; part of the output was published on Journal of Transport Geography, Transportation Research Part C: Emerging Technologies, Journal of Geomatics, and Journal of Geo-information Science.</p>
</li>
<li><p><strong>Data support for the <em>2015 ISPRS Scientific Initiative - Open Data Challenge</em> (participated project)</strong><br>Selected and preprocessed smart card data and bus GPS trajectory data for competition, detected the passengersâ€™ boarding and alighting stops as standard results for the required questions of the contest, cooperated with other members to design the scoring strategies and wrote description of given data and expected results. The international competition was successfully held in 2015.</p>
</li>
</ul>
</article></div></main><div class="nav-wrap"><div class="nav"><button class="site-nav"><div class="navicon"></div></button><ul class="nav_items"><li class="nav_item"><a class="nav-page" href="/"> HOME</a></li><li class="nav_item"><a class="nav-page" href="/publications"> PUBLICATIONS</a></li><li class="nav_item"><a class="nav-page" href="/blog"> BLOG</a></li><li class="nav_item"><a class="nav-page" href="/about"> ABOUT</a></li></ul></div><div class="cd-top"><i class="fa fa-arrow-up" aria-hidden="true"></i></div></div><footer id="page_footer"><div class="footer_wrap"><div class="copyright">&copy;2020 by Rui Cao (Ray)</div><div class="theme-info">Powered by <a href="https://hexo.io" target="_blank" rel="nofollow noopener">Hexo</a> & <a href="https://github.com/PhosphorW/hexo-theme-academia" target="_blank" rel="nofollow noopener">Academia Theme</a></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery-pjax@latest/jquery.pjax.min.js"></script><script src="/js/main.js"></script></body></html>